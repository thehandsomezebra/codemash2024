
... Back to [[index]] ...
[[Isn't it time to ditch code coverage- Focus on high value tests instead]]

```
Speaker 1   00:00
Living jobs. So there's actually super awesome to not have to worry about work or like speaking on behalf of any company here. Uh, today is it's very green. So if you ever get that opportunity, it'll feel good. Um, but anyway, uh, let's dive into The topic today. So here's a quick agenda, right? We're going to talk about this. Tradition metric. That is good coverage. Can't already tell from the title. It just might appear already. I don't like it. I think there's got to be something better. So we'll get into why it's not enough, and then it could right into the alternative, right? We're going to spend most of the time, only, an alternative. This is not going to be like a coat, heavy top, like how to write good. Remember like syntax organization standpoint, this is much higher level. There is a little bit of code. Just a peasy wall, but it's not going to be code heavy. So, if you come in here, hoping that being indicode and learn how to write all these different types of tests, that's not quite. What this is about, it is really about an alternative to code coverage, as a metric for accountability within your needs. So, Ever high volume tests if then get into a more comprehensive approach. So This, if anyone seen squid game, this is, oh, it is not. Pictures not showing. Wow, that's weird. Okay, I don't know what. There we go. So if anyone has seen squid game, uh, this thing will look familiar to you. Um, this kept, uh, popping into my head. As I thought about this topic and thinking about a lot of the engineering cultures that I've seen where quality is driven by this dreaded metric, that is code coverage. Those who haven't seen this. It is a shaft-up movie, it's really good, but it's really jacked up. Everyone's going through a challenge. There's 500 contestants. Someone at the end. I win a bunch of money. But that one person at the end. Spoiler alert. Sorry. Um, Is the only one who likes of the 500 contestants. So this is one of the first challenges where they have a, a square cookie and they have to carve a shape out of it without breaking. The cookie. And if they don't do it, they get shot in the head right there and, and they're dead. So I was just, this is what I was thinking about, like, The management Executives holding everyone accountable, the devs sweating it out, trying to hit that code coverage with every PR that they're open. And this is not the kind of engineering culture that we really want to be living in. At least, it's not the one I want. So I think there's something better and that's what we're going to get into. So, First things first. What is the right target? If we're measuring code coverage, we got to have a Target, right? That's what the metrics are for. It's accountability. So, I have a bunch of metrics here. I'm curious, just yell it out if you have a code coverage, uh, Target within your organization or your team. Just shouted out. I'd love to hear what some of you all are being held, accountable for 85. That's wow, I heard a lot of alien above, which is scary. I highlighted 70 here because that's kind of in my experience, uh, 17 minutes in this industry. Uh, that's kind of the average that I've seen. Although I've been in leadership, so I've kind of pushed it down lower. So, um, Maybe there's some of my influence in there. But I also heard a lot of different numbers. Which is. The biggest red flag to me. The whole purpose of an objective metric is to hold people accountable, and it should also have an objective Target to go along with it. You need to reach some point with your metric to say we've accomplished what we've tried to accomplish. And the fact that we heard so many different numbers and we see so many different numbers out. There is a red flag and this has led me to believe that most of these targets are really just Basically determined through a gut feel or the the dreaded phrase of well, that's just what we've always done. I I have personally never seen a Target in place that was driven by data and said, you know, based on all of our code and pull requests and all this data that been collected, we've determined that we hit the quality that we want. When we hit this code coverage, If any has anyone ever seen that? Yeah, so big red flag. And that's one of the biggest things that I think is wrong with code coverage. So there's, there's plenty more and we're gonna get into that now. So as you can see, there's a gimmicky, uh, little function over there called fake coverage. I hope no one is actually doing this, but it is proof that uh, you can actually dilute this this metric in. It's not showing. Yeah, it's demand is no. Um, Machines for a presentation, we'll do it. Thank you, holler out every time if it's doing that, it's probably going to do it on every slide that I have too much. But anyway, um This is a perfect example. Although done, uh, of how this metric is dilutable. It's a gamable metric. And when you have an objective metric, driving accountability, you don't really want it to be, uh, capable. So that's that's another red flag. The other thing that I think is wrong with code coverage, is that it creates all lines of code equally This could be a debate that we got in, but I I personally cannot believe all uh lines of coach at me. Treated equally. So, Really comes down to it from a product or company perspective when you look at it, where's your intellectual property? That's the value to the company to the product is, you're intellectual property. So that's going to be your domain logic or your business logic. That's where the real value is. So, you should be testing the hell out of that code. That's way more important than a bunch of, uh, boilerplate code, right? It's all necessary. But some of it is more important than other Code coverage, doesn't do what you do that. Also, another big thing, you'll probably hear me say this many times throughout this talk. Code coverage measures and how much of the code is tested, not how well it's tested. And those are two very, very, very different things. You'd have 20 more barriers in there. So that that is another big problem. And then finally, it's a mere correlation to Quality. It is not A causation, right? Those are not the same thing. There's white papers on this. I link to a white paper at the end that talks about the correlation kind of in favor of code coverage. So I don't want to hide that from you all and tell you that there's no zero value here or this is all just completely stupid but I I do think it is a poor metric and so with that said um we're going to need an alternative here um because otherwise it's really not worth talking about how Uncle Cracker metric is if we're not going to come up with an What is that alternative? Hopefully, you've read the title of the talk that you're decided to, but I attend here, it's right there in the subtitle it is. High value tests. So, and there we go again, there we go. So, high value Tess, I apologize that's gonna throw me off every single time. So, what the heck is a high volume test? We're, we're going to get into that, um, here and talk about some characteristics of how you value tests in general. And then we'll dive a little bit deeper into that. And so, this is not an exhaustive list of what makes a high value test. This is just a subset of some of the things that jumped out to me. The most I mean, is it has to serve a greater purpose than just working towards our code coverage, right? There's a lot of tests out there that get written of, I'm short on my code coverage. I'm just going to write a couple more tests to get it, and that is literally the mindset. And there's no thought whether that's really a good high value test. Is it meaningful? Is it just complete garbage? We're just chasing after this metric Excel. It needs to serve a greater purpose. The current average. Uh, high value test is going to be long lasting, not fragile, right? We all know how some of those poor. A test that we all end up writing. Uh, do not last long, they have to be updated and maintained constantly because they were just written important. So a good test will last longer and not be as fragile. Not saying it won't ever have to change. We all have to refactor stuff, and that will cause some changes, but it shouldn't be breaking all the time. Uh, another one here is, it needs to play to the strength of the type of the automated test that it is And where that's the part. We're going to dive into a little bit here, kept it in the middle to kind of highlight here that he's not in any certain order. But we'll get more into that one and these to run at the appropriate time in a timely manner. Under the appropriate circumstance. I think most of us if we've dealt with automated tests, we're well aware of that, right? It should be running in cicd in the right circumstances students test running on every commit on a pull request branch or just when it's merged or only in master or domain or whatever. We're doing. There's a lot of different places that different types of tests can run. We want to make sure that's right. Did it wrong? It takes down the value of those tests that are And then finally, it's got to be readable and well organized and you will have to revisit tests but hopefully it's not often and you want to be able to get in there quickly. Understand what's going on. For example, for using the fit on a unit test. The the AAA Arrange act, assert, that should be very obvious, which lines of code are going under each one of those A's there. So, it needs to be, well organized. It needs to allow other people who need to go update those tests and be able to do it quickly. Because frankly don't want to have to always spend so much time. Writing tests, we want to be writing more of the fun production code. So, With that said, let's dive a little bit deeper on what I refer to as these different types of tests. Hopefully, you all have seen these And written at least one or two of them. So at the top, we got into tests integration tests and unit tests for the purposes of this talk, I'm going to have a super loose liberal definition of integration. It's going to be for the most part. Anything that's not a test or in a test so uh, we'll get into that. But I do believe the unit SMH should be a smaller subset of tests. That immigration tests, immigration test is where you get a lot of bang for your buck. And we're going to dive into that. This is all dependable. If you want to talk to me about it, afterwards would love to South My strong opinions that I've developed over 17 years of my career. Let's dive in first to unit test. So, The biggest thing that I think makes a unit test. A high value test, is if we focus on business or domain logic, you can see over on the left here, we have just a simple layered application, right? It's got the front end. If we're doing a single page application night, it's got a API service layer, maybe it has a business, domain logic layer and then, of course, a data access layer. So I am very passionately believe, um, Makes a high value unit test is that it is focusing on business and domain logic. Like, I referred to earlier, right? That is where the intellectual property is in the product that you're building for whatever company you may be waiting for That's what's gonna give in the startup world, right? That's That's what's providing the value, that's how you get the valuation, right? So that's where you need to focus. That's some of your most important code. So you should unit test the help out of that area. You should make sure that that's right. Um, You got to do all these other things, right? You want them to be resilient. You want them to run all the time to make sure they're not working fast, executing in atomic, right? Which is another word for uniting. But I put that in there because I see so many tests in my experience that are claim to be unit tests but they are overly complex within one method. They're testing in so many different things and it's often kind of a red flag that maybe didn't code, its testing is so great. But back to the business, Kind of an example. There is I used to work on the SAS product that was for gymnastics competitions. So people would get in there organize, their gymnastics competition, and then they'd used the platform to execute the competition as well. And we had to build a basically a scoreboard for it. We had to calculate the scores in gymnastics. If any of you have ever been exposed to that world, it is super super complicated. One of the most complicated There. And the scoring for it is just as complicated. You have four events for women's six events for men, you have three different scoring algorithms, you can have attempts, Um, or not have attempts, you may have a single judge multiple judge. You can have a, I forget what it's called, like a primary judge that can even decide what other scores to drill out, super complicated. Um, in this application, our scoring calculators were I was about to say, the only place 90 of the unit tests we had in this fairly large application, were all just around this morning calculators, because we had to get that right there was nothing. That was gonna break down the trust of this application faster than not posting the score on time. Not posting the correct score or not posting the score at all right. No one's gonna start trusting that and they're all going to look. Um, Over at the judge's table where they hold a little piece of paper that I can't see from more than 20 feet away to try to find out that score because we got it wrong. So you have to get that right? On, on the flip side of this, right? We've all probably written a million cred API endpoints. I'm a firm believer that's not a great place to write unit tests, you don't get bang for your buck. You're gonna, if you do have a layered application here, you're going to be writing in a Tessa every layer just to make sure it's doing the same thing and you're going to spend a lot of time doing that, and it's really not that valuable, it's not going to help you. You can probably just knock that out with a simple uh integration test. Just hits all of those layers with one column. So I am going to get into an example here. Don't worry, I actually have the code here. I'm looking at a tiny image here. So this is just a simple program, is that big enough. Here there, ah, this is a simple program that is just has this function of fine for this points. So what we're trying to do here is take in a list of points and find the distance between the two points that are the furthest apart. If anyone went to the lightning talks, Last night, I really, really wanted to change this to find the distance between colors instead of just points, but that was going to be way too difficult and not enough time to do that. So a little disappointed, I didn't do that, but we're still on points here. Um, So it's you can see here down towards the bottom. We got our standard distance formula, But the the bulk of the effort here is in the the fine furthest points, we're just looping through points, comparing, calculating the distance, until we find the two points, better for this. And we return that this isn't um, That, that complicated. But let's say we did want to write some unit tests, if we are code coverage, motivated, We can solve that with one unit test here because we really only had one unhappy path defined here. Which is I have less than two points, right? Then we're going to throw an exception. If I write a very simple unit test that throws two points. They're two, totally different points. And I know what distance that is. I can assert that. That distance is whatever it is going to be and I'm going to get probably 80 code coverage here because there's only if you count curly braces, four lines of code that it doesn't hit with that one test. But are we going to be satisfied with that? I'm not cuz there I already know. There's a there's But if I'm code coverage motivated, I might just open the pull request and move on, because I've hit my Target and that's all I care about. And there are culture engineering cultures out there that do look like that. I've seen them time and time again, and so we don't want that. And so we're going to take the time And think about, okay, what are some high value tests? What are the tests that I need to write that are going to Build the confidence. That this works exactly how I am. I intend it to do. So, I actually came up with four tests, right? One of them is that unhappy path of less than two points, I just passing a point and I assert that the uh, operation interception is there, okay? And then I'm gonna uh, Do that one. That is gonna actually, I'm not gonna do that one yet. I'm gonna give it two points that are exactly the same. I expect that distance to be zero. That helps me know, that's just one more little case, that helps ensure that I got my distance formula correct. And then I'll move on. This is my like 80 code coverage one, right? I just give it two points. I know the distance is five, so I'm gonna assert another distance is five. That's going to hit our Target and we could move on, but we know we need more. Um, and then finally, you know, um, points in, I think it's quadrant three over Cartesian graph, right? Negative X negative y, uh, we want to make sure that that is working, at least one of those quadrants. So I, I knock it out. And have you both, uh, Of Y. That is going to be incorporated into the two points that are for this and make sure I still get a proper distance. With these four, what I would consider high value tests. That that gives me the confidence that I've done the job here and actually, you're probably going to lead to 100 code coverage but I don't care. That's not my motivation. It's kind of like a mess to have if you have to measure anymore, but the mentality is there to focus on high value tests rather than code coverage. This is just one example that kind of shows you could get into trouble if you remain focused on the the metric. So, We'll move on and talk to. About and in tests. So in the intests, right? They pretty much start at the front end. This is going to be a lot of your automated front-end tests aluminum Cypress. Playwright all of those that are typically at least in my experience written by QA test automation Engineers, not by software Engineers as much, although you still do see it. What I think makes these high value Is the fact that their workflow focused? Right. Unit tests, focused on the business, and domain logic into end tests. Our folks should be focused on the workflows that the users go through. I think one of the most valuable people that you could pair with, when it comes to writing into intests is a ux designer. They're the ones that have done, all the homework that Has determined which workflows are common and critical that the end users are going to go through. Hopefully, you can then build those out and you want to test those, that's where you're going to. Get the biggest bang for your buck, on the in test. You do not want to be single user interaction focused here because that doesn't prove to you that things work properly through an entire workflow, that your users are going to go to. So Is we all know that these types of tests are prone to be the most fragile, because the UI changes more often than all the other parts of an application. So you do just want to be workflow focused and not single user interaction focused. Uh, My little caveat here. I I said that working with ux designers on identifying these workflows and in user Journeys is really important. If you have not had the privilege of working with a really gifted ux designer, seek that opportunity, I come from an agency background and The vast majority of my clients put no value on ux designers they thought that they were simply UI designers in just design screens and that's it. They do a whole lot more and can bring a ton of value. And this is one of those areas that I think that they really pair with an engineering team to bring a lot of value. So I just like to preach about ux designers because I love them so much and they make my job so much better. So, Anyway. These do these do prone to be the slowest, executing tests, but we still need to make sure that we're writing tests that are fast. So these are still going to be fastest executing Because we all know the slower they go, the less often we run them and then it gets to the point where we don't run them at all. We'll just trust people to run them on their debt machines because that's going to work out. Super well not. Um, And then when you run, these is kind of dependent. Um I think uh it's good to run these before a PR merge circumstantially. You don't need to run them before every PR especially depending on how your code base is arranged, right? If you're just making back-end changes, you don't really execute those very often. So anyway workflow Focus, that's what I want. You all to walk away with for a high value limited test. All right. This one is a cop out. What makes a good integration test focusing on the areas that you feel are important. That didn't get covered by unit. Estimated tests, this is very subjective. I frankly had a hard time, pinning. This one down a little bit more. My passion really is and knows uh unit tests. Um, that's where I like to spend time, uh, testing. And I'll admit it. You can see it right there at the top. If I am accountable for some code coverage metric, this is what I focus on. This is how I get there. This is gonna. I've said it a couple times to give you the most bang for your buck. It should be the most common test that you have, um, because of how much your test or your code gets tested through these. Um, It's really great way to give more simple coverage, a little False sense of security. Um, In the the quality of your application. Um, They should be moderately resistant, right integration is is pretty loose. That could be between layers between components between surfaces, right? That's why I said it's real loose definition here. And if you have those things, they're probably communicating via interface or contract, there shouldn't be changed a whole lot. They do through factors in the future changes and stuff like that, but they should be moderately resistant resilient and they should be pretty fast. Executing not as fast as unit tests. But you still like I said, and the last one you have to focus on that mean executed quickly or they just going to die because you stop running them because Are backing up. So That's kind of my high level view of what. High volume tests are so The biggest question that's probably going through some of y'all's minds is okay. Well, if we have a metric that focuses on code coverage and we're going to dig it. What's the new metric, right? And we're saying, hey, focus on high value tests. So naturally, how do I measure The value of the test. So, How do we measure the value? This is where at this point all of you and reveal that we're not going to. If anyone has the answer to this, please raise your hand. I'm convinced that it doesn't exist. There's maybe an aggregation of many, many metrics that we could look at to, um, Value of our tests in our code, in the product. But there's no one single Magic Bullet, at least, not in the way that code coverage is where you can even automate the measurement of it. Very very simply with the cicb tool. You can probably get that done in a matter of minutes measuring the value. It just doesn't exist. So we're gonna have to come up with a different way unfortunately. So we're going to need a more comprehensive approach and this is probably going in a direction that y'all probably didn't see at all. So, taking a step back. I said, I've said this, I think a couple times code coverage is there to provide accountability. For the quality of the curve that be writing, or the product that you're you're building. That is the purpose, accountability, accountability, accountability. Most of the time at this metric, Lance on our responsibility because there's a perceived issue with the quality, right? If there's no perceived issue, People typically aren't asking you to measure it if they do, they're not even looking at the result because there's no perceived issue and they're just going to ignore it. So, we need something else. And so what I'm going to present to you here in the rest of this talk is that we need to shift our mindset from code coverage to actually building a culture of quality because if you do that, you're going to have the quality and no one's going to have a perceived issue of quality and they're not even going to ask you. So, Let's dive into that. What is culture? Just a couple questions to to answer here. So we want to build a culture of quality. What the heck is culture culture? Is an outcome of their values, at Community? Shares very, simply put small community, big Community. Obviously, the bigger, the community is, the more difficult. It is to come up with those shared values. But fortunately engineering teams aren't in the, you know, tens of thousands, unless you, you work for big Tech or something like that. Um, so it's not too difficult. To come up with some shared values around quality. So that's that's where we're gonna dig into, but we also have that question of, okay? How do we even establish a culture? What does that look like? So another quick definition here is, you know, it's the shared values in behaviors that exhibit those values that build up a culture so that's what we want to work towards. And along with that, we have to answer this question. Why does culture replace metrics? I kind of already hinted on it. A few slides ago when I started to get into this. But metrics often, not always try to resolve a culture problem. It goes back to saying, if there's a perceived issue, you're going to be asked to measure that issue. Set a goal so you can work towards it. It's all around accountability. And in my experience, That usually takes place when you have a weak culture in that area, that could be lots of different areas, right? It can be documentation, it can be Quality can be lots of things and until there's that perceived issue, no one's going to be knocking, on your door to go fix it. So let's try to make sure that that is not an issue to begin with and that's why I think culture can replace metrics and that's why we're going to work towards. Building that culture of quality. So, The first thing that we need to do is build up our values. This is not like your values on the silver platter. Every Community or team is different, you will have to get with your team and work on the values that you want to have in place to drive a culture of quality. But here's just three, small examples that we can kind of move forward with for the rest of the song. So, three values here. Hopefully, this you're not surprised by this first one, uh, given this presentation, right? We're gonna say that. We value. Uh, or we're going to lean into high value tests. Overcoat coverage, right? High value test. Trump code coverage. That's going to be one of our like principal values that is going to drive a lot of our behaviors. Additionally, we want to say that we value tests that the test code, the automated tests gets shipped with the code that is under test. There's a lot of teams out there that will push through the the code and then like in the next printful, right? All the automated tests because, well, they didn't account for it. They didn't get it done. Um, that Is okay, but it's not ideal. This still gives you. Whatever your test or your Sprint window, is that amount of time to Total box your system and have really poor quality. So we're going to value that they should together and then we're going to Value that automated test code is treated just like production code, and that's going to have some implications in our Behaviors as well. And so we're going to act. Like if we just have these three values that we're going to start building up other culture of quality. Again, you can come up with your, your own values. Are these I think these are decent starts but over time, they these values might mature Or morph a little bit and that's okay. You need to re-evaluate your values every once. So, Let's dive in, to the behaviors, to try to drive those values. Once again, just like the values, these are not, this is not an adjustable list, this is a starter. I've just come up with these on my own and through my experience and working with a lot of different teams. And I think these can start to very, very easily build up that culture of quality and put you on that path, to being able to feel very confident around the quality of the code and the quality of the product that you're you're building. So, first one here, include quality slice tests in your retros. When's the last time anyone talked about Quality in their Square retro. Okay, we have one person that may haven't done it in my recently, right? Couple hands. It's not often, right? We we don't ever focus on that add that as an agenda item in your retro to at least touch on it. Think about it, right? Um, That's going to help put everyone in the mindset of quality and we need that to be able to build our products in miserable chasing down, defects all the time. Another really important one here is document and reference your automated test. Best practices, right? You should be documenting what what is your definition of a high value test when it comes to unit tests and then tests integration tests, you have to document that because you have to make sure that that knowledge is known to the entire team so that when they're working on their tests. Um, they know what to work towards. And if you happen to be a code, reviewer on your PR and you see them miss the market sit, you know, that's just a comment in their PR and say hey this is a credit API endpoint. You wrote a lot of email tests. Really think that there's a more appropriate test. Here's the documentation. I'll take another step. Um include dedicated QA team members in my experience, there's always been this divide probably always will be and I hate it but uh, In my experience, we've mostly seen software Engineers, doing the integration and unit test and the QA test Engineers doing the front-end automated tests. I hate that divide and I think we should have always be trying to break down that divide and QA team members should be brought in on even the the back side of that, right? To help provide some accountability on the high value tests that you're trying to work through, right? Get Member's opinion of. Hey, here's here's what I have because this makes sense in terms of my unit tests or integration test. You think this covers all the right important things as we've discussed in and incorporate them as much as possible. Um, another idea here, if I've never seen this done, I think it'd be cool. But add it into your user stories or technical tasks or whatnot into the acceptance criteria, the tests that are expected, right? If you're writing a bunch of business logic, in your next task, Why not have some acceptance criteria in there? Saying we anticipate or there should be a decent Suite of unit tests associated with this to really call this accepted because it's quarter domain logic that we want to make sure is correct. Um, another one. Tesco reviewed in PRS. That in my experience just gets rubber stamped. Okay, you road test rubber stamps, no one even looks at that code whatsoever that's gonna not work. If we're aiming towards riding, how about the tests right there should be. That peer-to-peer accountability of looking at the tests in identifying, okay. Is this a good test? Is it clear? Um, If we're using AAA, like, I referred to, is it clear that? What lines of the code are the range? What lines are the act and which ones are the assert? We should look at that, right? It's really important that's going to continue to put us in that mindset of quality and working towards making those practices. That's going to lead to good quality. This one, I hope I don't have to speak about. Tests running within your cicd. That should be a given at this point. And if it's not yeah, just go ahead and get it done. If you haven't fallen into that Pitfall, I like I discussed earlier. If your tests are too slow so you backed off from one of them and now you're just hoping that the devs run them on their local machine. Get out of that mindset cuz trust me. They're not getting run at all. Um, do what you need to do to get them uh executing quickly? Kill off a bunch of deaths in that case to help it run quickly because you're not running them anyway. So if you kill them off a bunch off, that are running slow just so you can get a good starting point of running them at CI CD again. That's way better than what you had before because you weren't running back. That's a scary thing that I've proposed to people before. And they're like, nah, I can't delete them, they're super important. Well, the marketing branding them in cicd, they can't do that important. If you're not in South, make sure that's happening. Um, For tests and pointing your stories, right? If you're running scram and you're doing Sprint planning, you hopefully have an established velocity. Now, you're going to have to recognize, you're not getting as many stories done in the spread, because you have to start accounting for writing this automated tests. Within each story. And yeah it's going to slow you down but it's going to lead to Quality. And that the CPM convenient because he spent way less time focus on defects. So hopefully that's very broad knowledge. And people aren't still having that fight with management of. We need to write some tests here. I'm sure some of you probably still aren't and that's tragic, but work through that, push through that. Another thing that I have employed, and I think is a very powerful tool towards driving this culture. And some accountability is, including in your pull request templates, a test strategy, So that's one that we'll just go ahead and take a quick look at I have a QR code that gives a link. To this presentation. So you'll be able to click the get back and click the links and get access to this pull request template if you would like. But this is super simple, right? Nothing fancy. But it's gonna drive a lot of good behavior. Because, If you have a software engineer, Who's opening? We're about to open a pull request and they can see this template and they know they, they haven't written any tests yet, and they can't check any one of those three boxes and automated test strategy. They're probably gonna think Okay. I better go back and write some tests, otherwise whoever reviews this he's gonna call me out for it and I'm gonna have to do it anyway. So I'm gonna pause, I'm gonna go do that. Because value shipping tests with the code that is under test as well, lines up with the values that we've established. So I'm just gonna go ahead and hold myself accountable. Got a little nudge from the pr template and I'm gonna go do that. And if you're the reviewer same, same thing that you should see that. And as you can see, none of those checked you should say, hey, this doesn't align with our values. So I should nudge this person that decided to open the pr anyway and say, hey, um, took a quick scan through the code. Um, I I would be more than happy to dig into it further, once there's tests to go along with it, and I can review those tests as well, and make sure that they're Line up with our customer strategy. So very, very simple thing to put in place to drive accountability. So I'd recommend everyone do this because it's super loading fruit. So, That's the end of the behaviors that I have. I did want to open it up to see if anyone had any other behaviors that maybe popped front of mind that they'd like to share that would kind of lead to this values and building up that culture of life. I tried to think of most, but Anyone had any of that, but that popped out. And then maybe some behaviors that they're already doing in their teams would love to hear it and share it with them.

Speaker 2   41:35
Right, I covered them all this week.

Speaker 1   41:38
Okay. So, Isn't this all wonderful, right? We know exactly how to build a culture of quality. We can all walk out that door, go to our teams, get this done, drop that code of coverage metric and I'll be happy because we've gotten rid of a shooting method. Yeah, but There's probably heading a picture shut over the first time. Um, there's probably a lot of thoughts going on in your mind.

Speaker 3   42:13
That are similar to these.

Speaker 1   42:15
I know I've had all of these thoughts. Go through my mind. Many many times when it comes to this topic, right? Let's say you're just an individual contributor, I have no influence on this, I can't do it. That's the manager that needs to do that. Maybe you're stuck in this, uh, mentality. Then we just have to have an objective measurement for quality. Well, guess what? Yes, we had an objective metric with code coverage, but the target was the project. So we haven't had it to begin with, right? Maybe you're thinking changing a culture is just too hard. You would be right. It is well, maybe not too hard but it isn't really, really, freaking hard. I've spent I'm in engineering leadership. I've spent, um, Probably more of my time focused on culture and building cultures than writing any code. At least at this point in my career it is hard but you got to work towards it. And you can't even if you're an individual contributor. I would argue that while I have been Very focused on culture throughout my career. Uh, I would say, the vast majority of the culture that has been built up, is from individual contributors on my team. Right, you're gonna have influential people even if they're not. If they don't have authoritative uh, leadership, it just happens influential leadership. They're going to be capable of influencing that culture, so don't think that you can't drive this. Even if you're an individual entrepreneur, You know, the the two others here. Um, Are really tough, right? I'm

Speaker 2   43:58
Not

Speaker 1   43:58
Accountable for other people's behavior, right? That, that one's really tough. Um, I would argue that we all are accountable for each other's behavior. Unless no one is ever doing any cover reviews ever. And you're just writing your code and checking it straight in to get deployed. And then no one fusses. When you break the build, everyone just says that. It's all right, let's move on. Uh, That's not the real world. We're all doing code reviews. We're holding each other accountable all the time. Just not in all of the ways that we probably ought to So, I would argue that it is The job of everyone to hold each other accountable and that's going to build great High performing teams. In my experience. In engineering leadership. The number one thing people want within their teams are other high performers and that's what's going to keep them in their jobs and keep them. Happy is just having the privilege of working with other high performers. And you don't get that without having shared peer accountability. So finally, the other really tough one, my company demands code coverage from the top down. What the heck? Can you do about that? Sometimes you can't do anything at all and That might be. Okay, that doesn't prevent you from going on this adventure of building the culture quality, so that you don't have to really even worry about it. Worry about the code coverage. You may have to worry about it a little bit because naturally, just focusing on focusing on high value test, doesn't quite get you to your target and you might have to do a little more to get there. But that's just secondary. If you are able to get to that point where You don't have a Prestige issue with quality. Then great. Now you can go have that conversation. Hey, we're measuring this No one's looking at it. It's not useful. We have a culture of quality. There's no personal tissues here. Let's just get rid of it because it's just a waste of time and resources. Anyway and then, hopefully, you have someone that So don't let any of these things. Stop you from building a culture of quality within your teams or your company. It's imperative that you do because the alternative still doesn't work anyway, which is good leverage. Um, we'd all love to throw the code coverage probably into its fiery death. It has does anyone recognize this picture? Anyone we have one person, man. Not enough. People are on Tick Tock. Um, Chris went crazy viral as a Stanley Cup. Everyone should know. And maybe know what a Stanley Cup is. Uh, and it would still feel filled with ice water. You can view the ice in it. And this was after this car had been completely on fire. So I thought of this as well, like I want to throw code coverage into its firing death It's, it's like a Stanley Cup. It's like a cockroach, it's probably never going away. Um, but it doesn't mean it has to be riding Our Lives or our day-to-day

Speaker 2   47:15
Job. Um, we can realize it in the background focus on the culture and the high value tests instead. Um, and that's what I want to encourage everyone to do.

Speaker 1   47:26
Um, You're going to be much, much happier and hopefully not feel that. Same pressure

Speaker 2   47:31
Engineering culture,

Speaker 1   47:32
That kind of related to the the squid game picture at the beginning of this presentation. So with that said, go off start working on the culture start working on values and behaviors so that you can start working towards this.

Speaker 2   47:54
Any question. So I know I saw one over here. Yeah. Is there any value you can be a habit food separated or place? Um, I mean,

```
